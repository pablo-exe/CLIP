{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXGAZgKxODjb"
   },
   "source": [
    "# Búsqueda semántica inspirada en CLIP\n",
    "Selecciona las imágenes que se 'aproximan semánticamente' a un texto de búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75748,
     "status": "ok",
     "timestamp": 1694098364783,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "MR7re6K6ID7s",
    "outputId": "e8961671-7913-48cf-c803-2cf49dc14e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydantic 2.2.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.6.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "pip install -q -U tensorflow-hub tensorflow-text tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GCZGtF_OrzP"
   },
   "source": [
    "Bibliotecas que se utilizan en este programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13175,
     "status": "ok",
     "timestamp": 1694101763247,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "XvcZ4grTIL2L",
    "outputId": "9dcba413-e2ea-4158-c7ac-50b91426c128"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppressing tf.hub warnings\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-kVyYXHOqk0"
   },
   "source": [
    "We will use the MS-COCO dataset to train our dual encoder model. MS-COCO contains over 82,000 images, each of which has at least 5 different caption annotations. The dataset is usually used for image captioning tasks, but we can repurpose the image-caption pairs to train our dual encoder model for image search.\n",
    "\n",
    "Download and extract the data\n",
    "\n",
    "First, let's download the dataset, which consists of two compressed folders: one with images, and the other—with associated image captions. Note that the compressed images folder is 13GB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3854,
     "status": "ok",
     "timestamp": 1694100132164,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "HB2WLZQDIRi_",
    "outputId": "ada94a3f-b52c-4172-ce1c-a8f9b1118b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is downloaded and extracted successfully.\n",
      "Number of images: 82783\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"datasets\"\n",
    "annotations_dir = os.path.join(root_dir, \"annotations\")\n",
    "images_dir = os.path.join(root_dir, \"train2014\")\n",
    "tfrecords_dir = os.path.join(root_dir, \"tfrecords\")\n",
    "annotation_file = os.path.join(annotations_dir, \"captions_train2014.json\")\n",
    "\n",
    "# Download caption annotation files\n",
    "if not os.path.exists(annotations_dir):\n",
    "    annotation_zip = tf.keras.utils.get_file(\n",
    "        \"captions.zip\",\n",
    "        cache_dir=os.path.abspath(\".\"),\n",
    "        origin=\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
    "        extract=True,\n",
    "    )\n",
    "    os.remove(annotation_zip)\n",
    "\n",
    "# Download image files\n",
    "if not os.path.exists(images_dir):\n",
    "    image_zip = tf.keras.utils.get_file(\n",
    "        \"train2014.zip\",\n",
    "        cache_dir=os.path.abspath(\".\"),\n",
    "        origin=\"http://images.cocodataset.org/zips/train2014.zip\",\n",
    "        extract=True,\n",
    "    )\n",
    "    os.remove(image_zip)\n",
    "\n",
    "print(\"Dataset is downloaded and extracted successfully.\")\n",
    "\n",
    "with open(annotation_file, \"r\") as f:\n",
    "    annotations = json.load(f)[\"annotations\"]\n",
    "\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for element in annotations:\n",
    "    caption = f\"{element['caption'].lower().rstrip('.')}\"\n",
    "    image_path = images_dir + \"/COCO_train2014_\" + \"%012d.jpg\" % (element[\"image_id\"])\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "print(f\"Number of images: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoM3sar6Pb0U"
   },
   "source": [
    "<h3>Process and save the data to TFRecord files</h3><br>\n",
    "You can change the sample_size parameter to control many image-caption pairs will be used for training the dual encoder model. In this example we set train_size to 30,000 images, which is about 35% of the dataset. We use 2 captions for each image, thus producing 60,000 image-caption pairs. The size of the training set affects the quality of the produced encoders, but more examples would lead to longer training time.\n",
    "Esta porción de código nos importa menos para los objetivos de la asignatura. Se encuentra en el ámbito de 'Programación para Ciencia de Datos\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257281,
     "status": "ok",
     "timestamp": 1694100442070,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "xe6HiBgwIq42",
    "outputId": "18262fcc-5553-44b3-aa94-b5c9d8d4bea8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:42<00:00, 14.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 training examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:34<00:00, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 evaluation examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_size = 30000\n",
    "valid_size = 5000\n",
    "captions_per_image = 2\n",
    "images_per_file = 2000\n",
    "\n",
    "train_image_paths = image_paths[:train_size]\n",
    "num_train_files = int(np.ceil(train_size / images_per_file))\n",
    "train_files_prefix = os.path.join(tfrecords_dir, \"train\")\n",
    "\n",
    "valid_image_paths = image_paths[-valid_size:]\n",
    "num_valid_files = int(np.ceil(valid_size / images_per_file))\n",
    "valid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n",
    "\n",
    "tf.io.gfile.makedirs(tfrecords_dir)\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def create_example(image_path, caption):\n",
    "    feature = {\n",
    "        \"caption\": bytes_feature(caption.encode()),\n",
    "        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def write_tfrecords(file_name, image_paths):\n",
    "    caption_list = []\n",
    "    image_path_list = []\n",
    "    for image_path in image_paths:\n",
    "        captions = image_path_to_caption[image_path][:captions_per_image]\n",
    "        caption_list.extend(captions)\n",
    "        image_path_list.extend([image_path] * len(captions))\n",
    "\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for example_idx in range(len(image_path_list)):\n",
    "            example = create_example(\n",
    "                image_path_list[example_idx], caption_list[example_idx]\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "    return example_idx + 1\n",
    "\n",
    "\n",
    "def write_data(image_paths, num_files, files_prefix):\n",
    "    example_counter = 0\n",
    "    for file_idx in tqdm(range(num_files)):\n",
    "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
    "        start_idx = images_per_file * file_idx\n",
    "        end_idx = start_idx + images_per_file\n",
    "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
    "    return example_counter\n",
    "\n",
    "\n",
    "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
    "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
    "\n",
    "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
    "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-90Ohj4_P9mz"
   },
   "source": [
    "Create *tf.data.Dataset* for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1694100607195,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "hi99XHB2It16"
   },
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"caption\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "\n",
    "def read_example(example):\n",
    "    features = tf.io.parse_single_example(example, feature_description)\n",
    "    raw_image = features.pop(\"raw_image\")\n",
    "    features[\"image\"] = tf.image.resize(\n",
    "        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern, batch_size):\n",
    "\n",
    "    return (\n",
    "        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
    "        .map(\n",
    "            read_example,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False,\n",
    "        )\n",
    "        .shuffle(batch_size * 10)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZJRPMd_QJ-d"
   },
   "source": [
    "El modelo que vamos a crear es un *Dual Encoder*. El primer encoder recoge imágenes, mientras que el segundo encoder recoge los textos asociados a cada imagen. El resultado de ambos encoders va a ser comparado; para hacer esa comparación eficiente y adecuadamente, el formato de salida de ambos encoders debe coincidir, por ejemplo para realizar el producto escalar de ambos vectores. La función *project_embeddings* sirve para insertar unas capas como última etapa de cada uno de los dos decoders (imágenes y texto). Haciendo coincidir el tamaño de la última capa densa de ambos decoders, sus salidas serán comparables. <br>\n",
    "La línea 4 de la función crea una capa densa de tamaño *projection_dims* que recoge las entradas que se pasan como argumento a la función en su parámetro 'embeddings'. Este argumento será la salida del visual encoder, en un caso, y del text encoder en el otro. La profundidad del modelo que crea la función se establece en el argumento que se le pasa al parámetro 'num_projection_layers'. (línea 5). A mayor complejidad de la salida del encoder, mayor profundidad en este modelo. Cada capa de proyección incorpora una función de activación GeLu, una capa densa (de tamaño *projection_dims*), un dropout para regularizar, y una conexión que facilite que no se pierda la información de la capa anterior ('x') y que disminuya el efecto indeseado de 'vanishing gradient'. Finalmente se realiza una normalización.  \n",
    "<img src=CLIP.jpg width=80%></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1694101714805,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "hXsOIgy5Liw-"
   },
   "outputs": [],
   "source": [
    "def project_embeddings(\n",
    "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "):\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return projected_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR6DZ2z6WnFp"
   },
   "source": [
    "La función *create_vision_encoder* crea el encoder de las imágenes. Se toma el model *xception* como base de codificación (líneas 5 a 7) y se habilita su entrenamiento (líneas 9 y 10). Esto permite un adecuado fine tunning, pero a costa de un tiempo de procesamiento grande. El modelo completo recoge imágenes RGB de 299 x 299 (línea 12), las transforma al formato del modelo xception (línea 14), las traspasa al modelo xception (línea 16) y, finalmente, injecta su salida al modelo que hemos creado en la celda anterior (líneas 18 a 20), para asegurarnos de que su salida coincidirá en shape con la salida del decoder de texto. Esta función devuelve el modelo creado (línea 22)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1694101724594,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "FWgLWn5XLjBX"
   },
   "outputs": [],
   "source": [
    "def create_vision_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the pre-trained Xception model to be used as the base encoder.\n",
    "    xception = keras.applications.Xception(\n",
    "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = trainable\n",
    "    # Receive the images as inputs.\n",
    "    inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\")\n",
    "    # Preprocess the input image.\n",
    "    xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "    # Generate the embeddings for the images using the xception model.\n",
    "    embeddings = xception(xception_input)\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the vision encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"vision_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZJXejEYel4"
   },
   "source": [
    "La función *create_text_encoder* tiene la misma funcionalidad que *create_vision_encoder*, pero en este caso crea un embedding de los textos asociados a cada imagen, en lugar de un embedding de cada imagen. Mientras que en la función de visión usábamos el modelo xception, en el decoder de texto vamos a emplear BERT (líneas 5 a 13). Al igual que en la función anterior, aquí también habilitamos el entrenamiento de bert (línea 15). El resto de esta función es análogo (en texto) a la función anterior. Nótese que ambos encoders finalizan con el mismo modelo (creado con la función *project_embeddings*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1694101729546,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "anQHCKrFLjK_"
   },
   "outputs": [],
   "source": [
    "def create_text_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the BERT preprocessing module.\n",
    "    preprocess = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\",\n",
    "        name=\"text_preprocessing\",\n",
    "    )\n",
    "    # Load the pre-trained BERT model to be used as the base encoder.\n",
    "    bert = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "    name = \"bert\",\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    bert.trainable = trainable\n",
    "    # Receive the text as inputs.\n",
    "    inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    # Preprocess the text.\n",
    "    bert_inputs = preprocess(inputs)\n",
    "    # Generate embeddings for the preprocessed text using the BERT model.\n",
    "    embeddings = bert(bert_inputs)[\"pooled_output\"]\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the text encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbHIUqQfZ0CA"
   },
   "source": [
    "El núcleo de nuestro programa se implementa en la clase *DualEncoder*, que crea un modelo (hereda de keras.model, en la línea 1). En su constructor (línea 2) lo más relevante es que se le pasa el encoder de imágenes y el encoder de textos. Cuando se crea una instancia de la clase se invoca al método *call* que asigna a una GPU diferente cada encoder (dado el procesamiento requerido, se asume la existencia de dos GPUs)<br>\n",
    "La parte de entrenamiento de cada batch se procesa en la función *train_step*, que como es habitual invoca a *call* (vía *self*, en la línea 56). En nuestro caso, recibiendo los embeddings de texto y de imagen (*caption_embeddings, image_embeddings*); con estos embeddings se calcula la función de coste (línea 57) que se explicará después. De la manera estándard, a partir del loss se obtienen los gradientes (línea 59) y se actualizan los parámetros de la red neuronal (línea 60). La parte de testeo de cada batch se procesa en la función *test_step*, que realiza unas acciones similares a la función anterior, pero sin la parte de actualización de parámetros. <br><br>\n",
    "En este punto solo queda por explicar la manera en la que se obtiene el valor de *loss* a partir de ambos embeddings (el de la imagen y el de sus textos asociados). La función *compute_loss* (línea 24) realiza esta tarea. Para entender esta función hay que tener en cuenta que estamos realizando un aprendizaje contrastivo, y que las imágenes y textos de cada batch se cruzan para crear las muestras. Nuestra predicción se obtiene haciendo el producto escalar del embedding de la imagen con el embedding de los textos (líneas 26 a 29). Cuando el embedding de la imagen coincida con el embedding de los textos, se espera un valor alto. Esta predicción debe ser acorde con las imágenes y textos de la muestra. Si las imágenes y los textos son similares y nuestra prediccion dice que son similares, el loss mejora (o si las imágenes y textos no son similares y nuestra predicción indica que no lo son). Las imágenes las comparamos entre sí en las líneas 31 a 33; los textos en las líneas 35 a 37. En las líneas 39 a 41 simplemente promediamos los resultados anteriores. Finalmente, en las líneas 43 a 45, comparamos los valores reales (targets) con los predichos (logits). La función *crossentropy* de la línea 43 se encarga de proporcionar un valor bajo cuando los valores reales y las predicciones coinciden (baja entropía). Nota: *from_logits=True* simplemente informa de que los datos no estan normalizados probabilísticamente (no se les ha aplicado previamente la función softmax).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1694101855604,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "ki9Z1_FeLjRR"
   },
   "outputs": [],
   "source": [
    "class DualEncoder(keras.Model):\n",
    "    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        # Place each encoder on a separate GPU (if available).\n",
    "        # TF will fallback on available devices if there are fewer than 2 GPUs.\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            # Get the embeddings for the captions.\n",
    "            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n",
    "        with tf.device(\"/gpu:1\"):\n",
    "            # Get the embeddings for the images.\n",
    "            image_embeddings = vision_encoder(features[\"image\"], training=training)\n",
    "        return caption_embeddings, image_embeddings\n",
    "\n",
    "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
    "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
    "        logits = (\n",
    "            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n",
    "            / self.temperature\n",
    "        )\n",
    "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
    "        images_similarity = tf.matmul(\n",
    "            image_embeddings, image_embeddings, transpose_b=True\n",
    "        )\n",
    "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
    "        captions_similarity = tf.matmul(\n",
    "            caption_embeddings, caption_embeddings, transpose_b=True\n",
    "        )\n",
    "        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
    "        targets = keras.activations.softmax(\n",
    "            (captions_similarity + images_similarity) / (2 * self.temperature)\n",
    "        )\n",
    "        # Compute the loss for the captions using crossentropy\n",
    "        captions_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "        # Compute the loss for the images using crossentropy\n",
    "        images_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n",
    "        )\n",
    "        # Return the mean of the loss over the batch.\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            caption_embeddings, image_embeddings = self(features, training=True)\n",
    "            loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Monitor loss\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        caption_embeddings, image_embeddings = self(features, training=False)\n",
    "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmiXGlNvqUXB"
   },
   "source": [
    "En este texto creamos el encoder dual (el modelo completo) en la línea 10, e indicamos los parámetros de compilación del mismo (líneas 11 a 13). Al modelo *DualEncoder* (línea 10) hay que proporcionarle el encoder de vision y el de texto (constructor de la celda anterior (línea 2)). El *vision_encoder* se crea en las líneas 4 a 6, mientras que el *text_encoder* se creea en las líneas 7 a 9, empleando las funciones definidas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 22121,
     "status": "ok",
     "timestamp": 1694101888066,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "EFuzFGDPL3BC"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5  # In practice, train for at least 30 epochs\n",
    "batch_size = 256\n",
    "\n",
    "vision_encoder = create_vision_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "text_encoder = create_text_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n",
    "dual_encoder.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1H6H-nhru7C"
   },
   "source": [
    "Aquí se prepara todo para la ejecución del modelo (*fit*, en la línea 15). Se crean las variables *train_dataset* y *valid_dataset* con las muestras de entrenamiento y de testeo (líneas 5 y 6), y se establecen dos callabacks: *ReduceLROnPlateau* (líneas 8 a 10), e *EarlyStopping* (líneas 12 a 14)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxIL7mgZqTtk"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "executionInfo": {
     "elapsed": 631,
     "status": "error",
     "timestamp": 1694101892411,
     "user": {
      "displayName": "Jesus Bobadilla",
      "userId": "08927158501719244599"
     },
     "user_tz": -120
    },
    "id": "7FSGqnDNL3J_",
    "outputId": "e2ad4834-44d1-4a3d-ef95-0ee5d52b6f3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bcf095085de0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of examples (caption-image pairs): {train_example_count}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch size: {batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfrecords_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train-*.tfrecord\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_example_count' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(f\"Number of examples (caption-image pairs): {train_example_count}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\")\n",
    "train_dataset = get_dataset(os.path.join(tfrecords_dir, \"train-*.tfrecord\"), batch_size)\n",
    "valid_dataset = get_dataset(os.path.join(tfrecords_dir, \"valid-*.tfrecord\"), batch_size)\n",
    "# Create a learning rate scheduler callback.\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=3\n",
    ")\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "history = dual_encoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")\n",
    "print(\"Training completed. Saving vision and text encoders...\")\n",
    "vision_encoder.save(\"vision_encoder\")\n",
    "text_encoder.save(\"text_encoder\")\n",
    "print(\"Models are saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQY6y95bNxq7"
   },
   "source": [
    "Sacamos una gráfica de la evolución de la función de coste en entrenamiento y en testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ly0sdy9ENYP3"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhzv7YlOsgmu"
   },
   "source": [
    "Aquí generamos los embeddings de todas las imágenes (líneas 13 a 16), y los almacenamos en *image_embeddings*. Para obtener embeddings basta con hacer el feed forward (*predict*) en el modelo *vision_encoder* ya entrenado (línea 13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUXM9z-ysfTJ"
   },
   "outputs": [],
   "source": [
    "print(\"Loading vision and text encoders...\")\n",
    "vision_encoder = keras.models.load_model(\"vision_encoder\")\n",
    "text_encoder = keras.models.load_model(\"text_encoder\")\n",
    "print(\"Models are loaded.\")\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "    return tf.image.resize(image_array, (299, 299))\n",
    "\n",
    "\n",
    "print(f\"Generating embeddings for {len(image_paths)} images...\")\n",
    "image_embeddings = vision_encoder.predict(\n",
    "    tf.data.Dataset.from_tensor_slices(image_paths).map(read_image).batch(batch_size),\n",
    "    verbose=1,\n",
    ")\n",
    "print(f\"Image embeddings shape: {image_embeddings.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8Au6gwBsheL"
   },
   "source": [
    "La función *find_matches* encuentra las 'k' imágenes cuyos embeddings se aproximan más al embedding del texto de query. Para ello hay que obtener el embedding del texto haciendo uso de nuestro modelo *text_encoder* (línea 3). Tras una fase de normalización (líneas 6 y 7), se obtiene un vector que almacena los valores de similaridad entre el embedding del texto de query y cada uno de los embeddings de las imágenes disponibles (línea 9). En la línea 11 se extraen los índices de las 'k' imágenes cuyos embeddings se acercan más al del texto. Finalmente se devuelve una lista con todas esas imágenes (línea 13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QeeP-P8sfeZ"
   },
   "outputs": [],
   "source": [
    "def find_matches(image_embeddings, queries, k=9, normalize=True):\n",
    "    # Get the embedding for the query.\n",
    "    query_embedding = text_encoder(tf.convert_to_tensor(queries))\n",
    "    # Normalize the query and the image embeddings.\n",
    "    if normalize:\n",
    "        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n",
    "        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n",
    "    # Compute the dot product between the query and the image embeddings.\n",
    "    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n",
    "    # Retrieve top k indices.\n",
    "    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n",
    "    # Return matching image paths.\n",
    "    return [[image_paths[idx] for idx in indices] for indices in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cQKBf6-vLUo"
   },
   "source": [
    "Finalmente, utilizamos la función anterior (línea 2) con una query cualquiera (línea 1)  y dibujamos el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROrFcE0EsfoO"
   },
   "outputs": [],
   "source": [
    "query = \"a family standing next to the ocean on a sandy beach with a surf board\"\n",
    "matches = find_matches(image_embeddings, [query], normalize=True)[0]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(mpimg.imread(matches[i]))\n",
    "    plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMyr+C8gnyyGEnpx6lLdkEd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
